{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "# sys.path.insert(0, '../DevCode')\n",
    "\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.options.display.max_rows = 500\n",
    "sys.path.append('../src')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Read in the Scaled Data and Assign it </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('../src/data/scaled_test_features.npz')['a']\n",
    "y_test = np.load('../src/data/scaled_test_labels.npz')['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../src/data/scaled_train_features.npz')['a']\n",
    "y_train = np.load('../src/data/scaled_train_labels.npz')['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.load('../src/data/scaled_valid_features.npz')['a']\n",
    "y_val = np.load('../src/data/scaled_valid_labels.npz')['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> To help increase accuracy of the model, I reversed the sequences so that <br> the focus lies on the initial portions of the URL String and those important items become the end of the sequence</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:, ::-1].copy()\n",
    "X_val = X_val[:, ::-1].copy()\n",
    "X_test = X_test[:, ::-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> For the Model Architcture I desiced to use a Stacked LSTM. <br> \n",
    "The core LSTM feeds into a general Multi Layer Perceprtion and then utilizes <br>\n",
    "the ReLU activation function to help theoretically increase pattern recognition</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> For the Model Architcture I desiced to use a Bi Directional LSTM. <br> \n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # We multiply by 2 for bidirectional\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Assuming x is already in the correct shape: [batch_size, seq_len, input_dim]\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        out = self.fc2(hidden)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Assign and feature and label datasets and reshape them for LSTM Architecture</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train\n",
    "labels = y_train\n",
    "\n",
    "features = features.reshape(-1, 47,1)\n",
    "labels = labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load the dataset into our Data Loader clas </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert to tensors\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Assign our HyperParameters </h2>\n",
    "<h3>I decided to use a number of neurons equivelant to <br> \n",
    "twice the amount of samples in a sequence<br>\n",
    "<h3> I then selected a Step Scheduled Learning rate which would <br>\n",
    "cut the learning rate in half every 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 96\n",
    "output_dim = 4  # Assuming binary classification\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "learing_rate = 0.01\n",
    "\n",
    "\n",
    "model = BiLSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learing_rate)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=25, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../src/data/model2.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Perform our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.4716\n",
      "Epoch [2/300], Loss: 0.2897\n",
      "Epoch [3/300], Loss: 0.2014\n",
      "Epoch [4/300], Loss: 0.2678\n",
      "Epoch [5/300], Loss: 0.2281\n",
      "Epoch [6/300], Loss: 0.2933\n",
      "Epoch [7/300], Loss: 0.1862\n",
      "Epoch [8/300], Loss: 0.2039\n",
      "Epoch [9/300], Loss: 0.1755\n",
      "Epoch [10/300], Loss: 0.2792\n",
      "Epoch [11/300], Loss: 0.1681\n",
      "Epoch [12/300], Loss: 0.1253\n",
      "Epoch [13/300], Loss: 0.1533\n",
      "Epoch [14/300], Loss: 0.1534\n",
      "Epoch [15/300], Loss: 0.1701\n",
      "Epoch [16/300], Loss: 0.0980\n",
      "Epoch [17/300], Loss: 0.1888\n",
      "Epoch [18/300], Loss: 0.1163\n",
      "Epoch [19/300], Loss: 0.0971\n",
      "Epoch [20/300], Loss: 0.1406\n",
      "Epoch [21/300], Loss: 0.0820\n",
      "Epoch [22/300], Loss: 0.0855\n",
      "Epoch [23/300], Loss: 0.1572\n",
      "Epoch [24/300], Loss: 0.1258\n",
      "Epoch [25/300], Loss: 0.2171\n",
      "Epoch [26/300], Loss: 0.1454\n",
      "Epoch [27/300], Loss: 0.0602\n",
      "Epoch [28/300], Loss: 0.1244\n",
      "Epoch [29/300], Loss: 0.0952\n",
      "Epoch [30/300], Loss: 0.1116\n",
      "Epoch [31/300], Loss: 0.0893\n",
      "Epoch [32/300], Loss: 0.1099\n",
      "Epoch [33/300], Loss: 0.0984\n",
      "Epoch [34/300], Loss: 0.1318\n",
      "Epoch [35/300], Loss: 0.0897\n",
      "Epoch [36/300], Loss: 0.0928\n",
      "Epoch [37/300], Loss: 0.0844\n",
      "Epoch [38/300], Loss: 0.1071\n",
      "Epoch [39/300], Loss: 0.1062\n",
      "Epoch [40/300], Loss: 0.1077\n",
      "Epoch [41/300], Loss: 0.1019\n",
      "Epoch [42/300], Loss: 0.0864\n",
      "Epoch [43/300], Loss: 0.1113\n",
      "Epoch [44/300], Loss: 0.0715\n",
      "Epoch [45/300], Loss: 0.1104\n",
      "Epoch [46/300], Loss: 0.0855\n",
      "Epoch [47/300], Loss: 0.1073\n",
      "Epoch [48/300], Loss: 0.1087\n",
      "Epoch [49/300], Loss: 0.1041\n",
      "Epoch [50/300], Loss: 0.0838\n",
      "Epoch [51/300], Loss: 0.0735\n",
      "Epoch [52/300], Loss: 0.0857\n",
      "Epoch [53/300], Loss: 0.0963\n",
      "Epoch [54/300], Loss: 0.1408\n",
      "Epoch [55/300], Loss: 0.0800\n",
      "Epoch [56/300], Loss: 0.0789\n",
      "Epoch [57/300], Loss: 0.0484\n",
      "Epoch [58/300], Loss: 0.0467\n",
      "Epoch [59/300], Loss: 0.0658\n",
      "Epoch [60/300], Loss: 0.0610\n",
      "Epoch [61/300], Loss: 0.0679\n",
      "Epoch [62/300], Loss: 0.1006\n",
      "Epoch [63/300], Loss: 0.0797\n",
      "Epoch [64/300], Loss: 0.0825\n",
      "Epoch [65/300], Loss: 0.0839\n",
      "Epoch [66/300], Loss: 0.1035\n",
      "Epoch [67/300], Loss: 0.0638\n",
      "Epoch [68/300], Loss: 0.0842\n",
      "Epoch [69/300], Loss: 0.0825\n",
      "Epoch [70/300], Loss: 0.0459\n",
      "Epoch [71/300], Loss: 0.0743\n",
      "Epoch [72/300], Loss: 0.1798\n",
      "Epoch [73/300], Loss: 0.0633\n",
      "Epoch [74/300], Loss: 0.0405\n",
      "Epoch [75/300], Loss: 0.0893\n",
      "Epoch [76/300], Loss: 0.0605\n",
      "Epoch [77/300], Loss: 0.0787\n",
      "Epoch [78/300], Loss: 0.0743\n",
      "Epoch [79/300], Loss: 0.0399\n",
      "Epoch [80/300], Loss: 0.0800\n",
      "Epoch [81/300], Loss: 0.0566\n",
      "Epoch [82/300], Loss: 0.0475\n",
      "Epoch [83/300], Loss: 0.0552\n",
      "Epoch [84/300], Loss: 0.0437\n",
      "Epoch [85/300], Loss: 0.0506\n",
      "Epoch [86/300], Loss: 0.0623\n",
      "Epoch [87/300], Loss: 0.0231\n",
      "Epoch [88/300], Loss: 0.0378\n",
      "Epoch [89/300], Loss: 0.0113\n",
      "Epoch [90/300], Loss: 0.0665\n",
      "Epoch [91/300], Loss: 0.0644\n",
      "Epoch [92/300], Loss: 0.0642\n",
      "Epoch [93/300], Loss: 0.0335\n",
      "Epoch [94/300], Loss: 0.0294\n",
      "Epoch [95/300], Loss: 0.0367\n",
      "Epoch [96/300], Loss: 0.0654\n",
      "Epoch [97/300], Loss: 0.0349\n",
      "Epoch [98/300], Loss: 0.0335\n",
      "Epoch [99/300], Loss: 0.0292\n",
      "Epoch [100/300], Loss: 0.1287\n",
      "Epoch [101/300], Loss: 0.0782\n",
      "Epoch [102/300], Loss: 0.0340\n",
      "Epoch [103/300], Loss: 0.0225\n",
      "Epoch [104/300], Loss: 0.0474\n",
      "Epoch [105/300], Loss: 0.0159\n",
      "Epoch [106/300], Loss: 0.0138\n",
      "Epoch [107/300], Loss: 0.0456\n",
      "Epoch [108/300], Loss: 0.0153\n",
      "Epoch [109/300], Loss: 0.0465\n",
      "Epoch [110/300], Loss: 0.0301\n",
      "Epoch [111/300], Loss: 0.0534\n",
      "Epoch [112/300], Loss: 0.0204\n",
      "Epoch [113/300], Loss: 0.0159\n",
      "Epoch [114/300], Loss: 0.0378\n",
      "Epoch [115/300], Loss: 0.0184\n",
      "Epoch [116/300], Loss: 0.0178\n",
      "Epoch [117/300], Loss: 0.0249\n",
      "Epoch [118/300], Loss: 0.0458\n",
      "Epoch [119/300], Loss: 0.0482\n",
      "Epoch [120/300], Loss: 0.0338\n",
      "Epoch [121/300], Loss: 0.0491\n",
      "Epoch [122/300], Loss: 0.0163\n",
      "Epoch [123/300], Loss: 0.0483\n",
      "Epoch [124/300], Loss: 0.0227\n",
      "Epoch [125/300], Loss: 0.0164\n",
      "Epoch [126/300], Loss: 0.0258\n",
      "Epoch [127/300], Loss: 0.0349\n",
      "Epoch [128/300], Loss: 0.0213\n",
      "Epoch [129/300], Loss: 0.0337\n",
      "Epoch [130/300], Loss: 0.0206\n",
      "Epoch [131/300], Loss: 0.0316\n",
      "Epoch [132/300], Loss: 0.0263\n",
      "Epoch [133/300], Loss: 0.0543\n",
      "Epoch [134/300], Loss: 0.0177\n",
      "Epoch [135/300], Loss: 0.0064\n",
      "Epoch [136/300], Loss: 0.0284\n",
      "Epoch [137/300], Loss: 0.0184\n",
      "Epoch [138/300], Loss: 0.0187\n",
      "Epoch [139/300], Loss: 0.0290\n",
      "Epoch [140/300], Loss: 0.0394\n",
      "Epoch [141/300], Loss: 0.0306\n",
      "Epoch [142/300], Loss: 0.1810\n",
      "Epoch [143/300], Loss: 0.0258\n",
      "Epoch [144/300], Loss: 0.0179\n",
      "Epoch [145/300], Loss: 0.0628\n",
      "Epoch [146/300], Loss: 0.0138\n",
      "Epoch [147/300], Loss: 0.0056\n",
      "Epoch [148/300], Loss: 0.0231\n",
      "Epoch [149/300], Loss: 0.0291\n",
      "Epoch [150/300], Loss: 0.0395\n",
      "Epoch [151/300], Loss: 0.0076\n",
      "Epoch [152/300], Loss: 0.0350\n",
      "Epoch [153/300], Loss: 0.0279\n",
      "Epoch [154/300], Loss: 0.0317\n",
      "Epoch [155/300], Loss: 0.0331\n",
      "Epoch [156/300], Loss: 0.0307\n",
      "Epoch [157/300], Loss: 0.0182\n",
      "Epoch [158/300], Loss: 0.0311\n",
      "Epoch [159/300], Loss: 0.0463\n",
      "Epoch [160/300], Loss: 0.0291\n",
      "Epoch [161/300], Loss: 0.0307\n",
      "Epoch [162/300], Loss: 0.0360\n",
      "Epoch [163/300], Loss: 0.0190\n",
      "Epoch [164/300], Loss: 0.0139\n",
      "Epoch [165/300], Loss: 0.0119\n",
      "Epoch [166/300], Loss: 0.0215\n",
      "Epoch [167/300], Loss: 0.0304\n",
      "Epoch [168/300], Loss: 0.0297\n",
      "Epoch [169/300], Loss: 0.0197\n",
      "Epoch [170/300], Loss: 0.0144\n",
      "Epoch [171/300], Loss: 0.0198\n",
      "Epoch [172/300], Loss: 0.0431\n",
      "Epoch [173/300], Loss: 0.0142\n",
      "Epoch [174/300], Loss: 0.0248\n",
      "Epoch [175/300], Loss: 0.0298\n",
      "Epoch [176/300], Loss: 0.0403\n",
      "Epoch [177/300], Loss: 0.0223\n",
      "Epoch [178/300], Loss: 0.0378\n",
      "Epoch [179/300], Loss: 0.0075\n",
      "Epoch [180/300], Loss: 0.0477\n",
      "Epoch [181/300], Loss: 0.0146\n",
      "Epoch [182/300], Loss: 0.0053\n",
      "Epoch [183/300], Loss: 0.0241\n",
      "Epoch [184/300], Loss: 0.0175\n",
      "Epoch [185/300], Loss: 0.0233\n",
      "Epoch [186/300], Loss: 0.0276\n",
      "Epoch [187/300], Loss: 0.0180\n",
      "Epoch [188/300], Loss: 0.0288\n",
      "Epoch [189/300], Loss: 0.0179\n",
      "Epoch [190/300], Loss: 0.0706\n",
      "Epoch [191/300], Loss: 0.0099\n",
      "Epoch [192/300], Loss: 0.0300\n",
      "Epoch [193/300], Loss: 0.0182\n",
      "Epoch [194/300], Loss: 0.0297\n",
      "Epoch [195/300], Loss: 0.0208\n",
      "Epoch [196/300], Loss: 0.0173\n",
      "Epoch [197/300], Loss: 0.0220\n",
      "Epoch [198/300], Loss: 0.0094\n",
      "Epoch [199/300], Loss: 0.0102\n",
      "Epoch [200/300], Loss: 0.0420\n",
      "Epoch [201/300], Loss: 0.0371\n",
      "Epoch [202/300], Loss: 0.0343\n",
      "Epoch [203/300], Loss: 0.0293\n",
      "Epoch [204/300], Loss: 0.0132\n",
      "Epoch [205/300], Loss: 0.0182\n",
      "Epoch [206/300], Loss: 0.0296\n",
      "Epoch [207/300], Loss: 0.0372\n",
      "Epoch [208/300], Loss: 0.0290\n",
      "Epoch [209/300], Loss: 0.0386\n",
      "Epoch [210/300], Loss: 0.0161\n",
      "Epoch [211/300], Loss: 0.0226\n",
      "Epoch [212/300], Loss: 0.0332\n",
      "Epoch [213/300], Loss: 0.0066\n",
      "Epoch [214/300], Loss: 0.0140\n",
      "Epoch [215/300], Loss: 0.0335\n",
      "Epoch [216/300], Loss: 0.0117\n",
      "Epoch [217/300], Loss: 0.0092\n",
      "Epoch [218/300], Loss: 0.0191\n",
      "Epoch [219/300], Loss: 0.0192\n",
      "Epoch [220/300], Loss: 0.0253\n",
      "Epoch [221/300], Loss: 0.0287\n",
      "Epoch [222/300], Loss: 0.0094\n",
      "Epoch [223/300], Loss: 0.0164\n",
      "Epoch [224/300], Loss: 0.0195\n",
      "Epoch [225/300], Loss: 0.0605\n",
      "Epoch [226/300], Loss: 0.0102\n",
      "Epoch [227/300], Loss: 0.0222\n",
      "Epoch [228/300], Loss: 0.0184\n",
      "Epoch [229/300], Loss: 0.0459\n",
      "Epoch [230/300], Loss: 0.0195\n",
      "Epoch [231/300], Loss: 0.0138\n",
      "Epoch [232/300], Loss: 0.0325\n",
      "Epoch [233/300], Loss: 0.0094\n",
      "Epoch [234/300], Loss: 0.0230\n",
      "Epoch [235/300], Loss: 0.0201\n",
      "Epoch [236/300], Loss: 0.0256\n",
      "Epoch [237/300], Loss: 0.0199\n",
      "Epoch [238/300], Loss: 0.0071\n",
      "Epoch [239/300], Loss: 0.0157\n",
      "Epoch [240/300], Loss: 0.0327\n",
      "Epoch [241/300], Loss: 0.0210\n",
      "Epoch [242/300], Loss: 0.0104\n",
      "Epoch [243/300], Loss: 0.0261\n",
      "Epoch [244/300], Loss: 0.0173\n",
      "Epoch [245/300], Loss: 0.0233\n",
      "Epoch [246/300], Loss: 0.0137\n",
      "Epoch [247/300], Loss: 0.0159\n",
      "Epoch [248/300], Loss: 0.0263\n",
      "Epoch [249/300], Loss: 0.0254\n",
      "Epoch [250/300], Loss: 0.0101\n",
      "Epoch [251/300], Loss: 0.0233\n",
      "Epoch [252/300], Loss: 0.0130\n",
      "Epoch [253/300], Loss: 0.0264\n",
      "Epoch [254/300], Loss: 0.0155\n",
      "Epoch [255/300], Loss: 0.0248\n",
      "Epoch [256/300], Loss: 0.0214\n",
      "Epoch [257/300], Loss: 0.0295\n",
      "Epoch [258/300], Loss: 0.0223\n",
      "Epoch [259/300], Loss: 0.0683\n",
      "Epoch [260/300], Loss: 0.0377\n",
      "Epoch [261/300], Loss: 0.0247\n",
      "Epoch [262/300], Loss: 0.0208\n",
      "Epoch [263/300], Loss: 0.0448\n",
      "Epoch [264/300], Loss: 0.0130\n",
      "Epoch [265/300], Loss: 0.0270\n",
      "Epoch [266/300], Loss: 0.0160\n",
      "Epoch [267/300], Loss: 0.0140\n",
      "Epoch [268/300], Loss: 0.0251\n",
      "Epoch [269/300], Loss: 0.0308\n",
      "Epoch [270/300], Loss: 0.0142\n",
      "Epoch [271/300], Loss: 0.0112\n",
      "Epoch [272/300], Loss: 0.0450\n",
      "Epoch [273/300], Loss: 0.0211\n",
      "Epoch [274/300], Loss: 0.0492\n",
      "Epoch [275/300], Loss: 0.0309\n",
      "Epoch [276/300], Loss: 0.0253\n",
      "Epoch [277/300], Loss: 0.0254\n",
      "Epoch [278/300], Loss: 0.0220\n",
      "Epoch [279/300], Loss: 0.0096\n",
      "Epoch [280/300], Loss: 0.0155\n",
      "Epoch [281/300], Loss: 0.0472\n",
      "Epoch [282/300], Loss: 0.0505\n",
      "Epoch [283/300], Loss: 0.0198\n",
      "Epoch [284/300], Loss: 0.0135\n",
      "Epoch [285/300], Loss: 0.0139\n",
      "Epoch [286/300], Loss: 0.0171\n",
      "Epoch [287/300], Loss: 0.0177\n",
      "Epoch [288/300], Loss: 0.0329\n",
      "Epoch [289/300], Loss: 0.0128\n",
      "Epoch [290/300], Loss: 0.0547\n",
      "Epoch [291/300], Loss: 0.0199\n",
      "Epoch [292/300], Loss: 0.0376\n",
      "Epoch [293/300], Loss: 0.0305\n",
      "Epoch [294/300], Loss: 0.0262\n",
      "Epoch [295/300], Loss: 0.0262\n",
      "Epoch [296/300], Loss: 0.0132\n",
      "Epoch [297/300], Loss: 0.0482\n",
      "Epoch [298/300], Loss: 0.0137\n",
      "Epoch [299/300], Loss: 0.0364\n",
      "Epoch [300/300], Loss: 0.0142\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Forward pass\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        # inputs = inputs.cpu()\n",
    "        # labels = labels.cpu()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Save our Models and Optimizer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../src/data/model2.pt')\n",
    "torch.save(optimizer.state_dict(), '../src/data/optimizer2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Assign our Testing or Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bool = True\n",
    "\n",
    "if val_bool:\n",
    "    valid_features = X_val\n",
    "    valid_labels = y_val\n",
    "else:\n",
    "    valid_features = X_test\n",
    "    valid_labels = y_test\n",
    "\n",
    "\n",
    "valid_features = valid_features.reshape(-1, 47,1)\n",
    "valid_labels = valid_labels.reshape(-1)\n",
    "\n",
    "valid_features_tensor = torch.tensor(valid_features, dtype=torch.float32)\n",
    "valid_labels_tensor = torch.tensor(valid_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "valid_dataset = TensorDataset(valid_features_tensor, valid_labels_tensor)\n",
    "valid_data_loader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a function to run the testing or validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return val_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Run and analyze the testing or validation data and retrieve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0243, Validation Accuracy: 99.25%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "val_loss, val_accuracy = validate(model, valid_data_loader, criterion)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
