{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "# sys.path.insert(0, '../DevCode')\n",
    "\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.options.display.max_rows = 500\n",
    "sys.path.append('../code')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid = pd.read_pickle('../input/valid_tuple.pkl')\n",
    "# train = pd.read_pickle('../input/train_tuple.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid = pd.read_pickle('../code/data/valid_tuple.pkl')\n",
    "# train = pd.read_pickle('../code/data/train_tuple.pkl')\n",
    "# test = pd.read_pickle('../code/data/test_tuple.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_pickle('../code/data/scaled_valid_tuple.pkl')\n",
    "train = pd.read_pickle('../code/data/scaled_train_tuple.pkl')\n",
    "test = pd.read_pickle('../code/data/scaled_test_tuple.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train[0], train[1]\n",
    "X_val, y_val = valid[0], valid[1]\n",
    "X_test, y_test = test[0], test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:, ::-1].copy()\n",
    "X_val = X_val[:, ::-1].copy()\n",
    "X_test = X_test[:, ::-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.98896883e-06, 1.11586505e-05, 7.97200233e-06,\n",
       "       9.56451184e-06, 7.98264254e-06, 6.37624974e-06, 3.20083478e-06,\n",
       "       4.78255325e-06, 1.54816736e-05, 0.00000000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.18571932e-01, 1.11589173e-05,\n",
       "       1.08587879e-02, 6.37908680e-06, 4.53032544e-03, 6.37911732e-06,\n",
       "       2.22694427e-03, 6.39590662e-06, 4.86127181e-03, 1.11801798e-05,\n",
       "       1.11587394e-05, 2.88078818e-05, 2.55119938e-05])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.31240327e-05, 4.56646339e-02, 4.64222084e-05,\n",
       "       5.75314241e-01, 3.18989950e-06, 3.81340045e-01, 3.18955357e-06,\n",
       "       5.75079704e-01, 3.19214987e-06, 5.75075360e-01, 3.18826209e-06,\n",
       "       5.75071016e-01, 4.30588359e-05, 5.10099980e-05, 3.98694833e-05,\n",
       "       1.59409039e-05, 6.39590662e-06, 6.36465922e-02, 1.11801798e-05,\n",
       "       7.97052818e-06, 6.40175152e-06, 5.50102367e-04])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers,output_size=4):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=layers, batch_first=True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.1)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.layers, x.size(0), self.hidden_size).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.layers, x.size(0), self.hidden_size).requires_grad_().to(device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.fc(out[-1, :, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMAttentionClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Adapted for classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        out = self.fc(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StackedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "#         super(StackedLSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "    \n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifierNoEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMClassifierNoEmbedding, self).__init__()\n",
    "        # No embedding layer here\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # We multiply by 2 for bidirectional\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Assuming x is already in the correct shape: [batch_size, seq_len, input_dim]\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        out = self.fc2(hidden)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "        #dense_outputs = self.fc(hidden)\n",
    "        #return dense_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train\n",
    "labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440619,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440619, 47)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.reshape(-1, 47,1)\n",
    "labels = labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440619,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [0.00000000e+00],\n",
       "       [1.59409039e-06],\n",
       "       [6.39590662e-06],\n",
       "       [7.97976332e-06],\n",
       "       [6.38867416e-06],\n",
       "       [4.78231691e-06],\n",
       "       [3.20087576e-06],\n",
       "       [0.00000000e+00]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440619, 47, 1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "# features = np.random.randint(0, 100, (440619, 47))\n",
    "# labels = np.random.randint(0, 2, (440619,))\n",
    "\n",
    "# Convert to tensors\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "'1024'\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, optimizer, loss, criterion\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "# input_dim = 47\n",
    "\n",
    "# hidden_dim = 128\n",
    "# hidden_dim = 42\n",
    "\n",
    "hidden_dim = 46\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "output_dim = 4  # Assuming binary classification\n",
    "num_layers = 3\n",
    "# num_layers = 2\n",
    "\n",
    "learing_rate = 0.001\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# model = LSTM(input_size=input_dim, hidden_size=hidden_dim, layers=num_layers).to(device)\n",
    "\n",
    "# model = LSTMAttentionClassifier(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, num_classes=output_dim).to(device)\n",
    "\n",
    "model = StackedLSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, output_size=output_dim, dropout_rate=0.01).to(device)\n",
    "\n",
    "#model = BaseNN(input_size=47, hidden_size=256, output_size=output_dim).to(device)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "# model = LSTMClassifier(input_size=input_dim, hidden_size=hidden_dim, layers=num_layers,output_dim, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.0005)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hidden_dim = 46\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 128\n",
    "output_dim = 4  # Assuming binary classification\n",
    "num_layers = 3\n",
    "# num_layers = 2\n",
    "\n",
    "learing_rate = 0.001\n",
    "\n",
    "\n",
    "model = BiLSTMClassifierNoEmbedding(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=25, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifierNoEmbedding(\n",
      "  (lstm): LSTM(1, 128, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "14\n",
      "torch.Size([512, 1])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 1])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('../code/data/model.pt'))\n",
    "# optimizer.load_state_dict(torch.load('../code/data/optimizer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.param_groups[0]['lr'] = 0.0001\n",
    "# optimizer.param_groups[0]['lr'] = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '../code/data/model.pt')\n",
    "# torch.save(optimizer.state_dict(), '../code/data/optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.3406\n",
      "Epoch [2/300], Loss: 0.2706\n",
      "Epoch [3/300], Loss: 0.1776\n",
      "Epoch [4/300], Loss: 0.2319\n",
      "Epoch [5/300], Loss: 0.2015\n",
      "Epoch [6/300], Loss: 0.1982\n",
      "Epoch [7/300], Loss: 0.2195\n",
      "Epoch [8/300], Loss: 0.2214\n",
      "Epoch [9/300], Loss: 0.1749\n",
      "Epoch [10/300], Loss: 0.1831\n",
      "Epoch [11/300], Loss: 0.1419\n",
      "Epoch [12/300], Loss: 0.1655\n",
      "Epoch [13/300], Loss: 0.1028\n",
      "Epoch [14/300], Loss: 0.1022\n",
      "Epoch [15/300], Loss: 0.1594\n",
      "Epoch [16/300], Loss: 0.1841\n",
      "Epoch [17/300], Loss: 0.1233\n",
      "Epoch [18/300], Loss: 0.1786\n",
      "Epoch [19/300], Loss: 0.0868\n",
      "Epoch [20/300], Loss: 0.1280\n",
      "Epoch [21/300], Loss: 0.1442\n",
      "Epoch [22/300], Loss: 0.1592\n",
      "Epoch [23/300], Loss: 0.3096\n",
      "Epoch [24/300], Loss: 0.1196\n",
      "Epoch [25/300], Loss: 0.1496\n",
      "Epoch [26/300], Loss: 0.1864\n",
      "Epoch [27/300], Loss: 0.1379\n",
      "Epoch [28/300], Loss: 0.1200\n",
      "Epoch [29/300], Loss: 0.1124\n",
      "Epoch [30/300], Loss: 0.1271\n",
      "Epoch [31/300], Loss: 0.0917\n",
      "Epoch [32/300], Loss: 0.1446\n",
      "Epoch [33/300], Loss: 0.1098\n",
      "Epoch [34/300], Loss: 0.1347\n",
      "Epoch [35/300], Loss: 0.0817\n",
      "Epoch [36/300], Loss: 0.1685\n",
      "Epoch [37/300], Loss: 0.1035\n",
      "Epoch [38/300], Loss: 0.0753\n",
      "Epoch [39/300], Loss: 0.0916\n",
      "Epoch [40/300], Loss: 0.0657\n",
      "Epoch [41/300], Loss: 0.1477\n",
      "Epoch [42/300], Loss: 0.1030\n",
      "Epoch [43/300], Loss: 0.1296\n",
      "Epoch [44/300], Loss: 0.0865\n",
      "Epoch [45/300], Loss: 0.1388\n",
      "Epoch [46/300], Loss: 0.0928\n",
      "Epoch [47/300], Loss: 0.0832\n",
      "Epoch [48/300], Loss: 0.0789\n",
      "Epoch [49/300], Loss: 0.1437\n",
      "Epoch [50/300], Loss: 0.0724\n",
      "Epoch [51/300], Loss: 0.0973\n",
      "Epoch [52/300], Loss: 0.1035\n",
      "Epoch [53/300], Loss: 0.0933\n",
      "Epoch [54/300], Loss: 0.1257\n",
      "Epoch [55/300], Loss: 0.1183\n",
      "Epoch [56/300], Loss: 0.9048\n",
      "Epoch [57/300], Loss: 0.7231\n",
      "Epoch [58/300], Loss: 0.5180\n",
      "Epoch [59/300], Loss: 0.5097\n",
      "Epoch [60/300], Loss: 0.4701\n",
      "Epoch [61/300], Loss: 0.4595\n",
      "Epoch [62/300], Loss: 0.3811\n",
      "Epoch [63/300], Loss: 0.3463\n",
      "Epoch [64/300], Loss: 0.3904\n",
      "Epoch [65/300], Loss: 0.4246\n",
      "Epoch [66/300], Loss: 0.3420\n",
      "Epoch [67/300], Loss: 0.3003\n",
      "Epoch [68/300], Loss: 0.2465\n",
      "Epoch [69/300], Loss: 0.2835\n",
      "Epoch [70/300], Loss: 0.2393\n",
      "Epoch [71/300], Loss: 0.2592\n",
      "Epoch [72/300], Loss: 0.2492\n",
      "Epoch [73/300], Loss: 0.2891\n",
      "Epoch [74/300], Loss: 0.2498\n",
      "Epoch [75/300], Loss: 0.1983\n",
      "Epoch [76/300], Loss: 0.2185\n",
      "Epoch [77/300], Loss: 0.2294\n",
      "Epoch [78/300], Loss: 0.1838\n",
      "Epoch [79/300], Loss: 0.1927\n",
      "Epoch [80/300], Loss: 0.2233\n",
      "Epoch [81/300], Loss: 0.8936\n",
      "Epoch [82/300], Loss: 0.8658\n",
      "Epoch [83/300], Loss: 0.8203\n",
      "Epoch [84/300], Loss: 0.6538\n",
      "Epoch [85/300], Loss: 0.6358\n",
      "Epoch [86/300], Loss: 0.5644\n",
      "Epoch [87/300], Loss: 0.5144\n",
      "Epoch [88/300], Loss: 0.5060\n",
      "Epoch [89/300], Loss: 0.3886\n",
      "Epoch [90/300], Loss: 0.3913\n",
      "Epoch [91/300], Loss: 0.3594\n",
      "Epoch [92/300], Loss: 0.3994\n",
      "Epoch [93/300], Loss: 0.3423\n",
      "Epoch [94/300], Loss: 0.3830\n",
      "Epoch [95/300], Loss: 0.3457\n",
      "Epoch [96/300], Loss: 0.3677\n",
      "Epoch [97/300], Loss: 0.3791\n",
      "Epoch [98/300], Loss: 0.3389\n",
      "Epoch [99/300], Loss: 0.2448\n",
      "Epoch [100/300], Loss: 0.2981\n",
      "Epoch [101/300], Loss: 0.3762\n",
      "Epoch [102/300], Loss: 0.2675\n",
      "Epoch [103/300], Loss: 0.3006\n",
      "Epoch [104/300], Loss: 0.3013\n",
      "Epoch [105/300], Loss: 0.5140\n",
      "Epoch [106/300], Loss: 0.2842\n",
      "Epoch [107/300], Loss: 0.3295\n",
      "Epoch [108/300], Loss: 0.2811\n",
      "Epoch [109/300], Loss: 0.2859\n",
      "Epoch [110/300], Loss: 0.4826\n",
      "Epoch [111/300], Loss: 0.3827\n",
      "Epoch [112/300], Loss: 0.3445\n",
      "Epoch [113/300], Loss: 0.2898\n",
      "Epoch [114/300], Loss: 0.2546\n",
      "Epoch [115/300], Loss: 0.2691\n",
      "Epoch [116/300], Loss: 0.2463\n",
      "Epoch [117/300], Loss: 0.3309\n",
      "Epoch [118/300], Loss: 0.2078\n",
      "Epoch [119/300], Loss: 0.2330\n",
      "Epoch [120/300], Loss: 0.3251\n",
      "Epoch [121/300], Loss: 0.1714\n",
      "Epoch [122/300], Loss: 0.2932\n",
      "Epoch [123/300], Loss: 0.2357\n",
      "Epoch [124/300], Loss: 0.1477\n",
      "Epoch [125/300], Loss: 0.2101\n",
      "Epoch [126/300], Loss: 0.2569\n",
      "Epoch [127/300], Loss: 0.2182\n",
      "Epoch [128/300], Loss: 0.2564\n",
      "Epoch [129/300], Loss: 0.2069\n",
      "Epoch [130/300], Loss: 0.1780\n",
      "Epoch [131/300], Loss: 0.3479\n",
      "Epoch [132/300], Loss: 0.1957\n",
      "Epoch [133/300], Loss: 0.2622\n",
      "Epoch [134/300], Loss: 0.1676\n",
      "Epoch [135/300], Loss: 0.2112\n",
      "Epoch [136/300], Loss: 0.2463\n",
      "Epoch [137/300], Loss: 0.2124\n",
      "Epoch [138/300], Loss: 0.1924\n",
      "Epoch [139/300], Loss: 0.3256\n",
      "Epoch [140/300], Loss: 0.2307\n",
      "Epoch [141/300], Loss: 0.2446\n",
      "Epoch [142/300], Loss: 0.1887\n",
      "Epoch [143/300], Loss: 0.2359\n",
      "Epoch [144/300], Loss: 0.2097\n",
      "Epoch [145/300], Loss: 0.1787\n",
      "Epoch [146/300], Loss: 0.1728\n",
      "Epoch [147/300], Loss: 0.2116\n",
      "Epoch [148/300], Loss: 0.2106\n",
      "Epoch [149/300], Loss: 0.2402\n",
      "Epoch [150/300], Loss: 0.2455\n",
      "Epoch [151/300], Loss: 0.1351\n",
      "Epoch [152/300], Loss: 0.1455\n",
      "Epoch [153/300], Loss: 0.1957\n",
      "Epoch [154/300], Loss: 0.2651\n",
      "Epoch [155/300], Loss: 0.1373\n",
      "Epoch [156/300], Loss: 0.2584\n",
      "Epoch [157/300], Loss: 0.2092\n",
      "Epoch [158/300], Loss: 0.1636\n",
      "Epoch [159/300], Loss: 0.1868\n",
      "Epoch [160/300], Loss: 0.2466\n",
      "Epoch [161/300], Loss: 0.1744\n",
      "Epoch [162/300], Loss: 0.1272\n",
      "Epoch [163/300], Loss: 0.1772\n",
      "Epoch [164/300], Loss: 0.1929\n",
      "Epoch [165/300], Loss: 0.1782\n",
      "Epoch [166/300], Loss: 0.2524\n",
      "Epoch [167/300], Loss: 0.1656\n",
      "Epoch [168/300], Loss: 0.1546\n",
      "Epoch [169/300], Loss: 0.2349\n",
      "Epoch [170/300], Loss: 0.2152\n",
      "Epoch [171/300], Loss: 0.1375\n",
      "Epoch [172/300], Loss: 0.2021\n",
      "Epoch [173/300], Loss: 0.1990\n",
      "Epoch [174/300], Loss: 0.1736\n",
      "Epoch [175/300], Loss: 0.2088\n",
      "Epoch [176/300], Loss: 0.2060\n",
      "Epoch [177/300], Loss: 0.1677\n",
      "Epoch [178/300], Loss: 0.1368\n",
      "Epoch [179/300], Loss: 0.1340\n",
      "Epoch [180/300], Loss: 0.1863\n",
      "Epoch [181/300], Loss: 0.2778\n",
      "Epoch [182/300], Loss: 0.1943\n",
      "Epoch [183/300], Loss: 0.2351\n",
      "Epoch [184/300], Loss: 0.1813\n",
      "Epoch [185/300], Loss: 0.1897\n",
      "Epoch [186/300], Loss: 0.2076\n",
      "Epoch [187/300], Loss: 0.1539\n",
      "Epoch [188/300], Loss: 0.2088\n",
      "Epoch [189/300], Loss: 0.1247\n",
      "Epoch [190/300], Loss: 0.1380\n",
      "Epoch [191/300], Loss: 0.2285\n",
      "Epoch [192/300], Loss: 0.1825\n",
      "Epoch [193/300], Loss: 0.2048\n",
      "Epoch [194/300], Loss: 0.1944\n",
      "Epoch [195/300], Loss: 0.2340\n",
      "Epoch [196/300], Loss: 0.1798\n",
      "Epoch [197/300], Loss: 0.1884\n",
      "Epoch [198/300], Loss: 0.1959\n",
      "Epoch [199/300], Loss: 0.1930\n",
      "Epoch [200/300], Loss: 0.1414\n",
      "Epoch [201/300], Loss: 0.1958\n",
      "Epoch [202/300], Loss: 0.1368\n",
      "Epoch [203/300], Loss: 0.1724\n",
      "Epoch [204/300], Loss: 0.1612\n",
      "Epoch [205/300], Loss: 0.2106\n",
      "Epoch [206/300], Loss: 0.1573\n",
      "Epoch [207/300], Loss: 0.1584\n",
      "Epoch [208/300], Loss: 0.1650\n",
      "Epoch [209/300], Loss: 0.1359\n",
      "Epoch [210/300], Loss: 0.2082\n",
      "Epoch [211/300], Loss: 0.1825\n",
      "Epoch [212/300], Loss: 0.1864\n",
      "Epoch [213/300], Loss: 0.2466\n",
      "Epoch [214/300], Loss: 0.2507\n",
      "Epoch [215/300], Loss: 0.1531\n",
      "Epoch [216/300], Loss: 0.2467\n",
      "Epoch [217/300], Loss: 0.1801\n",
      "Epoch [218/300], Loss: 0.1526\n",
      "Epoch [219/300], Loss: 0.1722\n",
      "Epoch [220/300], Loss: 0.1358\n",
      "Epoch [221/300], Loss: 0.1577\n",
      "Epoch [222/300], Loss: 0.2254\n",
      "Epoch [223/300], Loss: 0.1844\n",
      "Epoch [224/300], Loss: 0.1892\n",
      "Epoch [225/300], Loss: 0.1196\n",
      "Epoch [226/300], Loss: 0.1856\n",
      "Epoch [227/300], Loss: 0.1919\n",
      "Epoch [228/300], Loss: 0.1560\n",
      "Epoch [229/300], Loss: 0.1820\n",
      "Epoch [230/300], Loss: 0.1024\n",
      "Epoch [231/300], Loss: 0.2002\n",
      "Epoch [232/300], Loss: 0.1563\n",
      "Epoch [233/300], Loss: 0.1159\n",
      "Epoch [234/300], Loss: 0.1580\n",
      "Epoch [235/300], Loss: 0.1875\n",
      "Epoch [236/300], Loss: 0.1633\n",
      "Epoch [237/300], Loss: 0.1521\n",
      "Epoch [238/300], Loss: 0.1489\n",
      "Epoch [239/300], Loss: 0.1487\n",
      "Epoch [240/300], Loss: 0.1295\n",
      "Epoch [241/300], Loss: 0.1626\n",
      "Epoch [242/300], Loss: 0.1438\n",
      "Epoch [243/300], Loss: 0.1735\n",
      "Epoch [244/300], Loss: 0.1601\n",
      "Epoch [245/300], Loss: 0.1188\n",
      "Epoch [246/300], Loss: 0.2599\n",
      "Epoch [247/300], Loss: 0.2208\n",
      "Epoch [248/300], Loss: 0.1355\n",
      "Epoch [249/300], Loss: 0.2046\n",
      "Epoch [250/300], Loss: 0.1324\n",
      "Epoch [251/300], Loss: 0.1932\n",
      "Epoch [252/300], Loss: 0.1350\n",
      "Epoch [253/300], Loss: 0.1804\n",
      "Epoch [254/300], Loss: 0.1481\n",
      "Epoch [255/300], Loss: 0.1562\n",
      "Epoch [256/300], Loss: 0.1871\n",
      "Epoch [257/300], Loss: 0.1420\n",
      "Epoch [258/300], Loss: 0.1798\n",
      "Epoch [259/300], Loss: 0.1298\n",
      "Epoch [260/300], Loss: 0.1324\n",
      "Epoch [261/300], Loss: 0.1819\n",
      "Epoch [262/300], Loss: 0.1879\n",
      "Epoch [263/300], Loss: 0.1584\n",
      "Epoch [264/300], Loss: 0.2212\n",
      "Epoch [265/300], Loss: 0.2057\n",
      "Epoch [266/300], Loss: 0.1733\n",
      "Epoch [267/300], Loss: 0.2162\n",
      "Epoch [268/300], Loss: 0.2674\n",
      "Epoch [269/300], Loss: 0.1983\n",
      "Epoch [270/300], Loss: 0.1975\n",
      "Epoch [271/300], Loss: 0.1868\n",
      "Epoch [272/300], Loss: 0.1343\n",
      "Epoch [273/300], Loss: 0.1181\n",
      "Epoch [274/300], Loss: 0.1727\n",
      "Epoch [275/300], Loss: 0.1819\n",
      "Epoch [276/300], Loss: 0.1811\n",
      "Epoch [277/300], Loss: 0.1704\n",
      "Epoch [278/300], Loss: 0.2390\n",
      "Epoch [279/300], Loss: 0.1643\n",
      "Epoch [280/300], Loss: 0.1711\n",
      "Epoch [281/300], Loss: 0.1350\n",
      "Epoch [282/300], Loss: 0.2098\n",
      "Epoch [283/300], Loss: 0.1486\n",
      "Epoch [284/300], Loss: 0.1763\n",
      "Epoch [285/300], Loss: 0.2220\n",
      "Epoch [286/300], Loss: 0.1497\n",
      "Epoch [287/300], Loss: 0.1481\n",
      "Epoch [288/300], Loss: 0.1515\n",
      "Epoch [289/300], Loss: 0.2125\n",
      "Epoch [290/300], Loss: 0.1772\n",
      "Epoch [291/300], Loss: 0.1869\n",
      "Epoch [292/300], Loss: 0.1388\n",
      "Epoch [293/300], Loss: 0.1805\n",
      "Epoch [294/300], Loss: 0.1363\n",
      "Epoch [295/300], Loss: 0.1772\n",
      "Epoch [296/300], Loss: 0.1782\n",
      "Epoch [297/300], Loss: 0.1927\n",
      "Epoch [298/300], Loss: 0.1694\n",
      "Epoch [299/300], Loss: 0.1905\n",
      "Epoch [300/300], Loss: 0.1520\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "# num_epochs = 150\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Forward pass\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        # inputs = inputs.cpu()\n",
    "        # labels = labels.cpu()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../code/data/model2.pt')\n",
    "torch.save(optimizer.state_dict(), '../code/data/optimizer2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../code/data/model_main.pt')\n",
    "# torch.save(optimizer.state_dict(), '../code/data/optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = X_test\n",
    "valid_labels = y_test\n",
    "\n",
    "valid_features = scaler.fit_transform(valid_features)\n",
    "\n",
    "valid_features = valid_features.reshape(-1, 47,1)\n",
    "valid_labels = valid_labels.reshape(-1)\n",
    "\n",
    "valid_features_tensor = torch.tensor(valid_features, dtype=torch.float32)\n",
    "valid_labels_tensor = torch.tensor(valid_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "valid_dataset = TensorDataset(valid_features_tensor, valid_labels_tensor)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bool = False\n",
    "\n",
    "if val_bool:\n",
    "    valid_features = X_val\n",
    "    valid_labels = y_val\n",
    "else:\n",
    "    valid_features = X_test\n",
    "    valid_labels = y_test\n",
    "\n",
    "\n",
    "valid_features = valid_features.reshape(-1, 47,1)\n",
    "valid_labels = valid_labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features_tensor = torch.tensor(valid_features, dtype=torch.float32)\n",
    "valid_labels_tensor = torch.tensor(valid_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "valid_dataset = TensorDataset(valid_features_tensor, valid_labels_tensor)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return val_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2285, Validation Accuracy: 94.61%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "val_loss, val_accuracy = validate(model, valid_data_loader, criterion)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
